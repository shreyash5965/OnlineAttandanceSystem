{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98a38286-6791-4da6-9c13-df32b0d7fc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capture images for Person1\n",
      "Saved: dataset\\Person1\\image0.jpg\n",
      "Capture images for Person2\n",
      "Saved: dataset\\Person2\\image0.jpg\n",
      "Capture images for Person3\n",
      "Saved: dataset\\Person3\\image0.jpg\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# Directory to save images\n",
    "data_dir = 'dataset'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "# Function to capture images\n",
    "def capture_images(person_name):\n",
    "    person_dir = os.path.join(data_dir, person_name)\n",
    "    if not os.path.exists(person_dir):\n",
    "        os.makedirs(person_dir)\n",
    "    \n",
    "    cap = cv2.VideoCapture(0)  # Adjust the camera index if necessary\n",
    "    count = 0\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        cv2.imshow('Capture Images', frame)\n",
    "        \n",
    "        # Press 'q' to quit, 's' to save an image\n",
    "        key = cv2.waitKey(1)\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "        elif key == ord('s'):\n",
    "            image_path = os.path.join(person_dir, f'image{count}.jpg')\n",
    "            cv2.imwrite(image_path, frame)\n",
    "            print(f'Saved: {image_path}')\n",
    "            count += 1\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Capture images for each person\n",
    "people = ['Person1', 'Person2', 'Person3']  # Add more as needed\n",
    "for person in people:\n",
    "    print(f'Capture images for {person}')\n",
    "    capture_images(person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfa2efbb-19c8-472c-ac53-91d3360cd906",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def collect_data(data_path):\n",
    "    images = []\n",
    "    labels = []\n",
    "    label_dict = {}\n",
    "    current_label = 0\n",
    "    \n",
    "    for person_name in os.listdir(data_path):\n",
    "        person_path = os.path.join(data_path, person_name)\n",
    "        if os.path.isdir(person_path):\n",
    "            label_dict[current_label] = person_name\n",
    "            for image_name in os.listdir(person_path):\n",
    "                image_path = os.path.join(person_path, image_name)\n",
    "                image = cv2.imread(image_path)\n",
    "                image = cv2.resize(image, (128, 128))\n",
    "                images.append(image)\n",
    "                labels.append(current_label)\n",
    "            current_label += 1\n",
    "    \n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "    return images, labels, label_dict\n",
    "\n",
    "data_path = 'dataset'\n",
    "images, labels, label_dict = collect_data(data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08929f59-72f5-4ce8-9a53-7a6e37356ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.3333 - loss: 35.0273\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 457ms/step - accuracy: 0.3333 - loss: -201.5885\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.3333 - loss: -504.8044\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.3333 - loss: -963.5918\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.3333 - loss: -1617.5219\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 373ms/step - accuracy: 0.3333 - loss: -2523.3652\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - accuracy: 0.3333 - loss: -3735.3313\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy: 0.3333 - loss: -5347.3008\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.3333 - loss: -7430.3296\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.3333 - loss: -10078.2070\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1fcbdd11be0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "def create_face_detection_model():\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')  # Binary classification (face or no face)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "detection_model = create_face_detection_model()\n",
    "detection_model.fit(images, labels, epochs=10, batch_size=32)  # Simplified training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cae66f61-3e5e-4d10-8a2b-d41d3476c3b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m     triplets \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(triplets)\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m triplets\n\u001b[1;32m---> 42\u001b[0m triplet_data \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_triplet_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m anchor_images \u001b[38;5;241m=\u001b[39m triplet_data[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     44\u001b[0m positive_images \u001b[38;5;241m=\u001b[39m triplet_data[:, \u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[1;32mIn[14], line 35\u001b[0m, in \u001b[0;36mgenerate_triplet_data\u001b[1;34m(images, labels)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(images)):\n\u001b[0;32m     34\u001b[0m     anchor \u001b[38;5;241m=\u001b[39m images[i]\n\u001b[1;32m---> 35\u001b[0m     \u001b[38;5;28mprint\u001b[39m(images[\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m])\n\u001b[0;32m     36\u001b[0m     positive \u001b[38;5;241m=\u001b[39m images[np\u001b[38;5;241m.\u001b[39mwhere(labels \u001b[38;5;241m==\u001b[39m labels[i])[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m]]  \u001b[38;5;66;03m# Another image of the same person\u001b[39;00m\n\u001b[0;32m     37\u001b[0m     negative \u001b[38;5;241m=\u001b[39m images[np\u001b[38;5;241m.\u001b[39mwhere(labels \u001b[38;5;241m!=\u001b[39m labels[i])[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]]  \u001b[38;5;66;03m# Image of a different person\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def triplet_loss(y_true, y_pred, alpha=0.2):\n",
    "    total_lenght = y_pred.shape.as_list()[-1]\n",
    "    anchor = y_pred[:, 0:int(total_lenght * 1/3)]\n",
    "    positive = y_pred[:, int(total_lenght * 1/3):int(total_lenght * 2/3)]\n",
    "    negative = y_pred[:, int(total_lenght * 2/3):int(total_lenght * 3/3)]\n",
    "    \n",
    "    pos_dist = K.sum(K.square(anchor - positive), axis=1)\n",
    "    neg_dist = K.sum(K.square(anchor - negative), axis=1)\n",
    "    \n",
    "    basic_loss = pos_dist - neg_dist + alpha\n",
    "    loss = K.maximum(basic_loss, 0.0)\n",
    "    return loss\n",
    "\n",
    "def create_face_recognition_model():\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
    "    x = base_model.output\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    model = Model(inputs=base_model.input, outputs=x)\n",
    "    return model\n",
    "\n",
    "recognition_model = create_face_recognition_model()\n",
    "recognition_model.compile(optimizer='adam', loss=triplet_loss)\n",
    "\n",
    "# Prepare triplet data for training\n",
    "def generate_triplet_data(images, labels):\n",
    "    triplets = []\n",
    "    for i in range(len(images)):\n",
    "        anchor = images[i]\n",
    "        print(images[np.where(labels == labels[i])[0][1]])\n",
    "        positive = images[np.where(labels == labels[i])[0][1]]  # Another image of the same person\n",
    "        negative = images[np.where(labels != labels[i])[0][0]]  # Image of a different person\n",
    "        triplets.append([anchor, positive, negative])\n",
    "    triplets = np.array(triplets)\n",
    "    return triplets\n",
    "\n",
    "triplet_data = generate_triplet_data(images, labels)\n",
    "anchor_images = triplet_data[:, 0]\n",
    "positive_images = triplet_data[:, 1]\n",
    "negative_images = triplet_data[:, 2]\n",
    "y_dummy = np.zeros((triplet_data.shape[0], 1))\n",
    "\n",
    "recognition_model.fit([anchor_images, positive_images, negative_images], y_dummy, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d971e041-df53-4cd9-91d6-37e70e43ecbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def mark_attendance(name):\n",
    "    with open('attendance.csv', 'r+') as f:\n",
    "        myDataList = f.readlines()\n",
    "        nameList = []\n",
    "        for line in myDataList:\n",
    "            entry = line.split(',')\n",
    "            nameList.append(entry[0])\n",
    "        if name not in nameList:\n",
    "            now = datetime.now()\n",
    "            dtString = now.strftime('%Y-%m-%d %H:%M:%S')\n",
    "            f.writelines(f'\\n{name},{dtString}')\n",
    "\n",
    "def recognize_faces(frame, detection_model, recognition_model, label_dict):\n",
    "    frame_resized = cv2.resize(frame, (128, 128))\n",
    "    faces = detection_model.predict(np.expand_dims(frame_resized, axis=0))\n",
    "    \n",
    "    if faces[0] > 0.5:  # Threshold for face detection\n",
    "        face_embedding = recognition_model.predict(np.expand_dims(frame_resized, axis=0))\n",
    "        min_dist = float('inf')\n",
    "        identity = None\n",
    "        \n",
    "        for label, name in label_dict.items():\n",
    "            known_face_embedding = recognition_model.predict(np.expand_dims(images[np.where(labels == label)[0][0]], axis=0))\n",
    "            dist = np.linalg.norm(face_embedding - known_face_embedding)\n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                identity = name\n",
    "        \n",
    "        if identity is not None:\n",
    "            mark_attendance(identity)\n",
    "            cv2.putText(frame, identity, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    \n",
    "    return frame\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    frame = recognize_faces(frame, detection_model, recognition_model, label_dict)\n",
    "    cv2.imshow('Webcam', frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
